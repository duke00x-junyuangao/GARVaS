---
title: "GARVaS: Genetic Algorithm for Regression Variable Selection"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{synopsis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Authors

* Yuan He
* Kuan-Cheng Lai
* Eugene Yedvabny

## Code

The source code is available on GitHub at https://github.com/eyedvabny/GARVaS and is provided as a fully-functional R package. If the `devtools` package is available, GARVaS is easily installed by running:

```{r, eval=FALSE}
devtools::install_github("eyedvabny/GARVaS")
```

The package depends on `doParallel`,`foreach` and `farway` packages, which will be installed alongside GARVaS.

## Introduction

GARVaS is an implementation of a Darwinian genetic algorithm for selecting best predictor variables for a linear or a generalized linear regression. Regression studies of multivariable datasets are often plagued by colinearity and confounding between the predictors, contributing to low predictive confidence.

A common approach to finding the best predictors in a large pool of candidates is an iterative process that starts with a full-interaction model and compares the quality of the fit after sequentially subtracting a predictor from the model. Unfortunately this process does not permute predictors, so good predictors can be eliminated without much improvement to the fitness if a bad predictor is still part of the model.

The genetic algorithm attemps to solve the above problem by keeping track of all the predictors and permuting them until the best combination emerges. Each predictor is a _gene_ on a _chromosome_ representing a model. A provided fitness function, e.g. AIC, determines the quality of that chromosome. At every generation the fittest chromosomes are stochastically crossed and mutated until "natural selection" converges on a _fittest individual_: a model with the best predictive power for the specified response.

## Code Layout

We have auxiliary functions to carry out initialization, mutation, crossover, selection of parents and production. Parallel processing is used in both production and selection of parents. Generic fitness function and a possible choice of glm() are included as options for the selection function.

Our main function select() takes advantage of the above auxiliary functions and does variable selection of the given dataset using fitness function and linear model chosen by users. Select() returns an S3 object with desired information stored as attributes. Printing and plotting methods are available for the S3 object.

## Testing

## Distribution of Work

Eugene did the R packages setup, initiated collaboration via Github, wrote the tests and improved the code.
Yuan wrote the algorithm and carried out the implementation.
Kuan-Cheng wrote the roxygen2 documentation and modified the algorithm.
The three of us finished the paper documentation together.

## Algorithm Implementation

```{r, echo=F}
library(GARVaS)
```

We want to implement our algorithm on the built-in dataset within R called  "mtcars", with the first column (mpg=miles per gallon) being the Y variable and the rest 10 columns being predictors.

First take a look at a portion of the dataset:

```{r, echo=F}
knitr::kable(head(mtcars))
```

Then we compare the full model with the updated model using the genetic algorithm. Since the dataset is small, we set generations to be 30 to maximize convergence.

```{r}
mod <- lm(mpg~., data = mtcars) #Full model
result <- select(mtcars, generations = 30)
print(result)
```

Then we want to take a look at the variables chosen by the algorithm:

```{r,echo=F}
if(ncol(result$best_genes) > 8){
  knitr::kable(result$best_genes[,1:8])
}else{
  knitr::kable(result$best_genes)
}
```

Only `r sum(result$best_genes[,1])` variables out of 10 were chosen, dimension got reduced significantly.

Now check the AIC and prediction accuracy of both models:

```{r}
AIC(mod)
AIC(result$model)

MSE <- sum((predict(mod)-mtcars[,1])^2)/nrow(mtcars)
MSE

MSE_new <- sum((predict(result$model)-mtcars[,1])^2)/nrow(mtcars)
MSE_new
```

We managed to decrease the AIC at the cost of sacrificing prediction accuracy (MSE became slightly bigger).

\pagebreak

Finally we plot to check normality of both models:

```{r, echo=FALSE}
plot(fitted(mod), residuals(mod), xlab="Fitted Value",
     ylab="Residual(Error)", main="Residual Plot (full)")
plot(fitted(result$model), residuals(result$model), xlab="Fitted Value", 
     ylab="Residual(Error)", main="Residual Plot (updated)")
```

\pagebreak

```{r, echo=FALSE}
qqnorm(residuals(mod), main="Normal Q-Q Plot (full)")
qqline(residuals(mod))
qqnorm(residuals(result$model), main="Normal Q-Q Plot (updated)")
qqline(residuals(result$model))
```

Applying genetic algorithm impairs normality of the model by introducing patterns to residuals. This is reasonable because by throwing away most of the predictors, we are trading completeness for conciseness.
